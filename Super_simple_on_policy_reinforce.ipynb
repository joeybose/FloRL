{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import mujoco_py\n",
    "import os\n",
    "import gym\n",
    "import ipdb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, hidden_size, action_space):\n",
    "        \n",
    "        super(Policy,self).__init__()\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.linear = nn.Linear(num_inputs,hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, num_outputs)\n",
    "        self.log_std = nn.Linear(hidden_size, num_outputs)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "            \n",
    "        x = inputs\n",
    "        x = F.relu(self.linear(x))\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        \n",
    "        log_std = self.log_std(x) \n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX) #clamp log std into range\n",
    "        std = log_std.exp()\n",
    "            \n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, hidden_dim):                                                                                                                                                             \n",
    "        super(ValueNetwork, self).__init__()                                                                                                                                                                \n",
    "                                                                                                                                                                                                            \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)                                                                                                                                                    \n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)                                                                                                                                                    \n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = F.relu(self.linear1(state))                                                                                                                                                                     \n",
    "        x = F.relu(self.linear2(x))                                                                                                                                                                         \n",
    "        x = self.linear3(x)                                                                                                                                                                                 \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \n",
    "    def __init__(self, num_inputs, hidden_size, action_space, lr_pi = 1e-2,\\\n",
    "                 gamma = 0.99, baseline = False, lr_vf = 1e-2):\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.policy = Policy(num_inputs, hidden_size, action_space)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr = lr_pi)\n",
    "        self.gamma = gamma\n",
    "        self.baseline = False\n",
    "        \n",
    "        if baseline:\n",
    "            self.baseline = True\n",
    "            self.value_function = ValueNetwork(num_inputs, hidden_size)\n",
    "            self.value_optim = optim.Adam(self.value_function.parameters(), lr = lr_vf)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "            \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0) # just to make it a Tensor obj\n",
    "        \n",
    "        # get mean and std\n",
    "        mean, std = self.policy(state)\n",
    "        # create normal distribution\n",
    "        normal = Normal(mean, std)\n",
    "        # sample action\n",
    "        action = normal.sample()\n",
    "        # get log prob of that action\n",
    "        ln_prob = normal.log_prob(action)\n",
    "        ln_prob = ln_prob.sum()\n",
    "        \n",
    "        # squeeze action into [-1,1]\n",
    "        action = torch.tanh(action)\n",
    "        # turn actions into numpy array\n",
    "        action = action.numpy()\n",
    "\n",
    "        return action[0], ln_prob, mean, std\n",
    "    \n",
    "    \n",
    "    def train(self, log_probs, rewards):\n",
    "        \n",
    "        R = 0\n",
    "        returns = []\n",
    "        \n",
    "        for r in rewards[::-1]:\n",
    "            \n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.tensor(returns) # rewards to go for each step of env trajectory\n",
    "        \n",
    "        if self.baseline:\n",
    "            value_estimates = []\n",
    "            \n",
    "            \n",
    "            \n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "            \n",
    "        policy_loss = torch.stack( policy_loss ).sum()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()                                                                                                                                                                          \n",
    "        policy_loss.backward()                                                                                                                                                                                     \n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss\n",
    "    \n",
    "    def train2 (self, trajectory):\n",
    "        \n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "    \n",
    "        for t in trajectory:\n",
    "    \n",
    "            state, action, ln_prob, reward, next_state, done = t\n",
    "    \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(ln_prob)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "###################### calculate rewards to go #####################################\n",
    "        R = 0\n",
    "        returns = []\n",
    "        \n",
    "        for r in rewards[::-1]:\n",
    "            \n",
    "            R = r + 0.99 * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.tensor(returns) # rewards to go for each step of env trajectory\n",
    "        \n",
    "## #############################get value function loss ##########################################\n",
    "\n",
    "        value_estimates = []\n",
    "        for state in states:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0) # just to make it a Tensor obj\n",
    "            value_estimates.append( self.value_function(state) )\n",
    "\n",
    "\n",
    "        value_estimates = torch.stack(value_estimates).squeeze() # rewards to go for each step of env trajectory\n",
    "\n",
    "        v_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "        self.value_optim.zero_grad()                                                                                                                                                                          \n",
    "        v_loss.backward()                                                                                                                                                                                     \n",
    "        self.value_optim.step()\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "        advantage = []\n",
    "        for value, R in zip(value_estimates, returns):\n",
    "\n",
    "            advantage.append(R - value)\n",
    "        advantage = torch.Tensor(advantage)\n",
    "# get policy loss #\n",
    "        policy_loss = []\n",
    "        for log_prob, adv in zip(log_probs, advantage):\n",
    "            policy_loss.append(-log_prob * adv)\n",
    "            \n",
    "        policy_loss = torch.stack( policy_loss ).sum()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()                                                                                                                                                                          \n",
    "        policy_loss.backward()                                                                                                                                                                                     \n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "#         return loss\n",
    "        \n",
    "        \n",
    "#     def train(self, trajectory):                                                                                                                                                                            \n",
    "                                                                                                                                                                                                            \n",
    "#         '''                                                                                                                                                                                                 \n",
    "#         trajectory: a list of the form                                                                                                                                                                     \n",
    "#         [(lnP(a_t|s_t), r(s_t,a_t) ),(lnP(a_{t+1}|s_{t+1}), r(s_{t+1},a_{t+1}))]                                                                                                                            \n",
    "                                                                                                                                                                                                            \n",
    "#         Train the model by summing lnP(a_t|s_t)*r(s_t,a_t)                                                                                                                                                  \n",
    "#         '''                                                                                                                                                                                                 \n",
    "                                                                                                                                                                                                            \n",
    "#         loss = 0                                                                                                                                                                                        \n",
    "#         for step in trajectory:                                                                                                                                                                             \n",
    "#             # look at one step                                                                                                                                                                              \n",
    "#             ln_prob, reward = step \n",
    "#             loss = loss - ln_prob * reward \n",
    "                                                                                                                                          \n",
    "#         self.optimizer.zero_grad()                                                                                                                                                                          \n",
    "#         loss.backward()                                                                                                                                                                                     \n",
    "#         self.optimizer.step()\n",
    "        \n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-08 14:25:13,707] Making new env: MountainCarContinuous-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of actions:Box(1,), dim of states: 2, max_action:[1.], min_action: [-1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/pward/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\") #\n",
    "#env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "#env = gym.make(\"Pendulum-v0\")\n",
    "#env = gym.make(\"InvertedPendulum-v1\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space\n",
    "max_action = env.action_space.high\n",
    "min_action = env.action_space.low\n",
    "\n",
    "print(\"number of actions:{0}, dim of states: {1}, max_action:{2}, min_action: {3}\".format(action_dim,state_dim,max_action,min_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create comet an experiment\n",
    "# experiment = Experiment(api_key=\"BUXbNT79Q2PEtRkuX9swzxspZ\",\n",
    "#                         project_name=\"florl\", workspace=\"nadeem-ward\")\n",
    "\n",
    "# experiment.set_name(\"FLORL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes = 10):\n",
    "    '''\n",
    "        function to return the average reward of the policy over 10 runs\n",
    "    '''\n",
    "    \n",
    "    avg_reward = 0.0\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action, log_prob, mean, std = policy.select_action(np.array(obs))\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            avg_reward += reward\n",
    "            \n",
    "    avg_reward /= eval_episodes\n",
    "    \n",
    "    print(\"the average reward is: {0}\".format(avg_reward))\n",
    "    #return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(policy):\n",
    "    '''\n",
    "        Function to see the policy in action\n",
    "    '''\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action,_,_,_ = policy.select_action(np.array(obs))\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing everything and update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode:1\n",
      "At episode:2\n",
      "At episode:3\n",
      "At episode:4\n",
      "At episode:5\n",
      "At episode:6\n",
      "At episode:7\n",
      "At episode:8\n",
      "At episode:9\n",
      "At episode:10\n",
      "At episode:11\n",
      "At episode:12\n",
      "At episode:13\n",
      "At episode:14\n",
      "At episode:15\n",
      "At episode:16\n",
      "At episode:17\n",
      "At episode:18\n",
      "At episode:19\n",
      "At episode:20\n",
      "At episode:21\n",
      "At episode:22\n",
      "At episode:23\n",
      "At episode:24\n",
      "At episode:25\n",
      "At episode:26\n",
      "At episode:27\n",
      "At episode:28\n",
      "At episode:29\n",
      "At episode:30\n",
      "At episode:31\n",
      "At episode:32\n",
      "At episode:33\n",
      "At episode:34\n",
      "At episode:35\n",
      "At episode:36\n",
      "At episode:37\n",
      "At episode:38\n",
      "At episode:39\n",
      "At episode:40\n",
      "At episode:41\n",
      "At episode:42\n",
      "At episode:43\n",
      "At episode:44\n",
      "At episode:45\n",
      "At episode:46\n",
      "At episode:47\n",
      "At episode:48\n",
      "At episode:49\n",
      "At episode:50\n",
      "At episode:51\n",
      "At episode:52\n",
      "At episode:53\n",
      "At episode:54\n",
      "At episode:55\n",
      "At episode:56\n",
      "At episode:57\n",
      "At episode:58\n",
      "At episode:59\n",
      "At episode:60\n",
      "At episode:61\n",
      "At episode:62\n",
      "At episode:63\n",
      "At episode:64\n",
      "At episode:65\n",
      "At episode:66\n",
      "At episode:67\n",
      "At episode:68\n",
      "At episode:69\n",
      "At episode:70\n",
      "At episode:71\n",
      "At episode:72\n",
      "At episode:73\n",
      "At episode:74\n",
      "At episode:75\n",
      "At episode:76\n",
      "At episode:77\n",
      "At episode:78\n",
      "At episode:79\n",
      "At episode:80\n",
      "At episode:81\n",
      "At episode:82\n",
      "At episode:83\n",
      "At episode:84\n",
      "At episode:85\n",
      "At episode:86\n",
      "At episode:87\n",
      "At episode:88\n",
      "At episode:89\n",
      "At episode:90\n",
      "At episode:91\n",
      "At episode:92\n",
      "At episode:93\n",
      "At episode:94\n",
      "At episode:95\n",
      "At episode:96\n",
      "At episode:97\n",
      "At episode:98\n",
      "At episode:99\n",
      "At episode:100\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "policy = REINFORCE (state_dim, hidden_size, action_dim, baseline = True)\n",
    "\n",
    "\n",
    "max_episodes = 100\n",
    "total_episodes = 0    \n",
    "while total_episodes < max_episodes:\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    trajectory = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action, ln_prob, mean, std = policy.select_action(np.array(obs))\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        trajectory.append( [np.array(obs), action, ln_prob, reward, next_state, done] )\n",
    "        log_probs.append(ln_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        #trajectory_info.append([ln_prob, reward])\n",
    "            #print(\"state:{0}, next_state:{1}, reward:{2}, action{3}, done:{4}\".format(obs,next_state, reward, action,done))\n",
    "        obs = next_state\n",
    "\n",
    "    total_episodes += 1\n",
    "        \n",
    "    print(\"At episode:{0}\".format(total_episodes))\n",
    "    #print(log_probs)\n",
    "    #print(rewards)\n",
    "    #policy.train(log_probs, rewards)\n",
    "    policy.train2(trajectory)\n",
    "    #experiment.log_metric(\"Loss Value\", value_loss, step = total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average reward is: -13.787747109819424\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing problem with type \n",
    "\n",
    "# Lunar Lander NEEDS actions between -1,1\n",
    "\n",
    "# action needs to be numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 1\n",
    "total_episodes = 0 \n",
    "\n",
    "policy = REINFORCE (state_dim, hidden_size, action_dim, baseline = True)\n",
    "\n",
    "while total_episodes < max_episodes:\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    trajectory = []\n",
    "    \n",
    "    while not done:\n",
    "        # select action\n",
    "        action, ln_prob, mean, std = policy.select_action(np.array(obs))\n",
    "        # use action on state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # append info\n",
    "        trajectory.append( [np.array(obs),action,ln_prob,reward,next_state, done] )\n",
    "\n",
    "        obs = next_state\n",
    "\n",
    "    total_episodes += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = []\n",
    "rewards = []\n",
    "states = []\n",
    "actions = []\n",
    "\n",
    "for t in trajectory:\n",
    "    \n",
    "    state, action, ln_prob, reward, next_state, done = t\n",
    "    \n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    log_probs.append(ln_prob)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    \n",
    "###################### calculate rewards to go #####################################\n",
    "R = 0\n",
    "returns = []\n",
    "        \n",
    "for r in rewards[::-1]:\n",
    "            \n",
    "    R = r + 0.99 * R\n",
    "    returns.insert(0, R)\n",
    "            \n",
    "returns = torch.tensor(returns) # rewards to go for each step of env trajectory\n",
    "        \n",
    "## #############################get value function loss ##########################################\n",
    "\n",
    "value_estimates = []\n",
    "for state in states:\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0) # just to make it a Tensor obj\n",
    "    value_estimates.append( policy.value_function(state) )\n",
    "\n",
    "#print(value_estimates)\n",
    "value_estimates = torch.stack(value_estimates).squeeze() # rewards to go for each step of env trajectory\n",
    "#print(returns.shape)\n",
    "#print(value_estimates.shape)\n",
    "#print(value_estimates)\n",
    "#print(value_estimates.squeeze())\n",
    "\n",
    "v_loss = F.mse_loss(value_estimates, returns)\n",
    "\n",
    "policy.value_optim.zero_grad()                                                                                                                                                                          \n",
    "v_loss.backward()                                                                                                                                                                                     \n",
    "policy.value_optim.step()\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "advantage = []\n",
    "for value, R in zip(value_estimates, returns):\n",
    "\n",
    "    advantage.append(R - value)\n",
    "\n",
    "# policy.value_function = ValueNetwork(num_inputs, hidden_size)\n",
    "# policy.value_optim = optim.Adam(self.value_function.parameters(), lr = lr_vf)\n",
    "\n",
    "# get policy loss #\n",
    "policy_loss = []\n",
    "for log_prob, adv in zip(log_probs, advantage):\n",
    "    policy_loss.append(-log_prob * adv)\n",
    "            \n",
    "policy_loss = torch.stack( policy_loss ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
